Challenge Overview
Challenge Objective
Develop an AI Agent that can analyse static code review results from tools like SonarQube and answer a set of NLP questions based on these results which are specific to the challenge being reviewed by the agent.

Tech Stack
Node.js (Typescript) or Python
LLM
Sonarqube
Inputs
The AI Agent will be provided with following inputs and should use those to compile comprehensive submission code review according to the requirements:

Static Code Review Results: JSON or XML output from tools like SonarQube.
NLP Questions: A set of predefined questions related to the code review results. Note that questions may have weights associated to them such that sum up to 100% but define importance of a question in the set. Example of the question structure is a Topcoder review scorecard: https://software.topcoder.com/review/actions/ViewScorecard?scid=30002133
Actual code: The code base, which is being reviewed, provided as .zip archive file containing the code base.
Challenge specification text: Includes description of challenge requirements, deliverables, evaluation criteria etc.
List of technologies and skills: Forming the stack used.
Outputs
Screening result for each submission
Answers to NLP Questions: The AI agent will provide answers based on the static code review results.
Each answer/response should include a quality rating score form 0 – 5 representing the confidence level of the generated answer.
Output Format: It should be in JSON format with proper mapping question <-> review comment.
Functional Requirements
Data Ingestion: Ability to ingest static code review results from SonarQube, .zip of actual code which is under review.
Data Screening: Ability to perform basic input data screening. In case of providing empty .zip without source code or such with nonsense files, not real code base, or unrelated code base for misuse, agent should be able to understand if the code base does not relate to challenge specification and act accordingly.
NLP Processing: Ability to understand and process natural language questions.
Answer Generation: Ability to generate answers based on the code review results in context of challenge specification, actual code base etc.
LLM Selection
Model: Choose a lightweight open - source LLM which should be optimized for quick inference times.
Deployment: Ensure the agent model can be deployed on a local server or cloud server (preferably AWS) with limited resources.
Cost: The model and all associated tools should be open - source and free to use.
Deliverables
Source code implemented in TypeScript/JS.
Documentation
Full installation guide with how to deploy, configure and use the provided software solution.
Several examples of the agent in action with generated reports should be included in submission for review. Note
Evaluation Criteria
Accuracy of Responses:
a. The AI agent should provide accurate answers to the predefined NLP questions based on the static code review results and challenge context.
b. Commit to design & implement advanced prompt engineering which provides best quality results.
c. Consider setting a benchmark accuracy percentage (e.g., 90% or above, configurable).
Performance and Speed:
a. The AI agent should process and respond to queries within a specified time frame (e.g., less than 2 seconds per query).
b. Evaluate the model's efficiency in terms of resource usage, especially if deployed on limited - resource servers.
Integration and Compatibility:
a. The AI agent must be able to seamlessly ingest static code review results from SonarQube in the specified formats (JSON or XML).
b. Evaluate the integration process with your existing systems or workflows.
Documentation and Examples:
a. Assess the completeness and clarity of the provided documentation and installation guides.
b. Verify the quality and relevance of the examples of the agent in action.
Implementation Requirements
Use the LangChain.js framework for prompt templating, LLM wrapper, document loading when implementing the agent runtime.
You are encouraged to design and implement Multi - Agent AI Architecture. A system comprised from specialized AI agents, each handling a distinct part of the review process, will have likely better performance compared to single AI Agent responsible for everything – seek “divide and conquer” strategy. Finally, implement a Master AI Reviewer Agent which combines output of all sub - agents for final decision making and report generation.
Your solution should consider context length of the LLM, and handle the inputs properly when the sonar report and actual code exceeds the context length.
References
SonarQube Scanning Examples - https://github.com/sonarsource/sonar-scanning-examples?tab=readme-ov-file
Sonar Web API results data:
https://next.sonarqube.com/sonarqube/web_api/api/projects?query=export
https://next.sonarqube.com/sonarqube/web_api/api/issues?query=export
Try out SonarQube Server tutorial (totally FREE of charge) will help you evaluate SonarQube Server and generate project results for sample code base which you can use to tweak the AI Review Agent for better quality: https://docs.sonarsource.com/sonarqube-server/10.8/try-out-sonarqube/
When you got the server running you will need the SonarScanner CLI for scanning code: https://docs.sonarsource.com/sonarqube-community-build/analyzing-source-code/scanners/sonarscanner/#configuring-your-project
When executed, above will generate the data in Sonar which you could then export via the Web API and use it as AI agent context.
Rules & Guidelines
Submissions must be original work
Use only publicly available or appropriately licensed resources
Include all required components for submission eligibility
Get Started!
This is your chance to push the boundaries of AI in real world problems! We can't wait to see what you create. :rocket:
If you have any questions, feel free to ask in the challenge forum.
